1)Download vechile sales data -> https://github.com/shashank-mishra219/Hive-Class/blob/main/sales_order_data.csv
downloaded.

2)Store raw data into hdfs location
done.

3)Create a internal hive table "sales_order_csv" which will store csv data sales_order_csv .. make sure to skip header row while creating table

hive> create table sales_order_csv
    >  (ORDERNUMBER int,
    > QUANTITYORDERED int,
    > PRICEEACH float,
    > ORDERLINENUMBER int,
    > SALES float,
    > STATUS string,
    > QTR_ID int,
    > MONTH_ID int, YEAR_ID int,
    > PRODUCTLINE string,
    > MSRP int,
    > PRODUCTCODE string,
    > PHONE string,
    > CITY string,
    > STATE string,
    > POSTALCODE string,
    > COUNTRY string,
    > TERRITORY string,
    > CONTACTLASTNAME string,
    > CONTACTFIRSTNAME string,
    > DEALSIZE string
    > )
    > row format delimited
    > fields terminated by ','
    > tblproperties("skip.header.line.count"="1");
OK


4)Load data from hdfs path into "sales_order_csv" 


hive> load data local inpath 'file:///bin/sales_order_data.csv' into table sales_order_csv;
Loading data to table hive_db.sales_order_csv
OK
Time taken: 0.736 seconds

hive> set hive.cli.print.header = true;
hive> select  * from sales_order_csv limit 10;
OK
sales_order_csv.ordernumber     sales_order_csv.quantityordered sales_order_csv.priceeach       sales_order_csv.orderlinenumber sales_order_csv.sales   sales_order_csv.status  sales_order_csv.qtr_id  sales_order_csv.month_id   sales_order_csv.year_id sales_order_csv.productline     sales_order_csv.msrp    sales_order_csv.productcode     sales_order_csv.phone   sales_order_csv.city    sales_order_csv.state   sales_order_csv.postalcode sales_order_csv.country sales_order_csv.territory       sales_order_csv.contactlastname sales_order_csv.contactfirstname        sales_order_csv.dealsize
10107   30      95.7    2       2871.0  Shipped 1       2       2003    Motorcycles     95      S10_1678        2125557818      NYC     NY      10022   USA     NA      Yu      Kwai    Small
10121   34      81.35   5       2765.9  Shipped 2       5       2003    Motorcycles     95      S10_1678        26.47.1555      Reims           51100   France  EMEA    Henriot Paul    Small
10134   41      94.74   2       3884.34 Shipped 3       7       2003    Motorcycles     95      S10_1678        +33 1 46 62 7555        Paris           75508   France  EMEA    Da Cunha        Daniel  Medium
10145   45      83.26   6       3746.7  Shipped 3       8       2003    Motorcycles     95      S10_1678        6265557265      Pasadena        CA      90003   USA     NA      Young   Julie   Medium
10159   49      100.0   14      5205.27 Shipped 4       10      2003    Motorcycles     95      S10_1678        6505551386      San Francisco   CA              USA     NA      Brown   Julie   Medium
10168   36      96.66   1       3479.76 Shipped 4       10      2003    Motorcycles     95      S10_1678        6505556809      Burlingame      CA      94217   USA     NA      Hirano  Juri    Medium
10180   29      86.13   9       2497.77 Shipped 4       11      2003    Motorcycles     95      S10_1678        20.16.1555      Lille           59000   France  EMEA    Rance   Martine Small
10188   48      100.0   1       5512.32 Shipped 4       11      2003    Motorcycles     95      S10_1678        +47 2267 3215   Bergen          N 5804  Norway  EMEA    Oeztan  Veysel  Medium
10201   22      98.57   2       2168.54 Shipped 4       12      2003    Motorcycles     95      S10_1678        6505555787      San Francisco   CA              USA     NA      Murphy  Julie   Small
10211   41      100.0   14      4708.44 Shipped 1       1       2004    Motorcycles     95      S10_1678        (1) 47.55.6555  Paris           75016   France  EMEA    Perrier Dominique       Medium
Time taken: 0.122 seconds, Fetched: 10 row(s)
hive>

5)Create an internal hive table which will store data in ORC format "sales_order_orc"


hive> create table sales_order_csv_orc
    >  (ORDERNUMBER int,
    > QUANTITYORDERED int,
    > PRICEEACH float,
    > ORDERLINENUMBER int,
    > SALES float,
    > STATUS string,
    > QTR_ID int,
    > MONTH_ID int, YEAR_ID int,
    > PRODUCTLINE string,
    > MSRP int,
    > PRODUCTCODE string,
    > PHONE string,
    > CITY string,
    > STATE string,
    > POSTALCODE string,
    > COUNTRY string,
    > TERRITORY string,
    > CONTACTLASTNAME string,
    > CONTACTFIRSTNAME string,
    > DEALSIZE string
    > )
    > Stored as ORC;
OK
Time taken: 0.09 seconds

6). Load data from "sales_order_csv" into "sales_order_orc"

hive> from sales_order_csv insert overwrite table sales_order_csv_orc select *;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = root_20230620164535_dde1ad10-0c0c-4834-bfe5-1577f9f6f993
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Job running in-process (local Hadoop)
2023-06-20 16:45:37,042 Stage-1 map = 100%,  reduce = 0%
Ended Job = job_local310119218_0001
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to directory hdfs://namenode:9000/user/hive/warehouse/hive_db.db/sales_order_csv_orc/.hive-staging_hive_2023-06-20_16-45-35_191_7923960465826923610-1/-ext-10000
Loading data to table hive_db.sales_order_csv_orc
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 368425 HDFS Write: 397965 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
sales_order_csv.ordernumber     sales_order_csv.quantityordered sales_order_csv.priceeach       sales_order_csv.orderlinenumber sales_order_csv.sales   sales_order_csv.status  sales_order_csv.qtr_id  sales_order_csv.month_id   sales_order_csv.year_id sales_order_csv.productline     sales_order_csv.msrp    sales_order_csv.productcode     sales_order_csv.phone   sales_order_csv.city    sales_order_csv.state   sales_order_csv.postalcode sales_order_csv.country sales_order_csv.territory       sales_order_csv.contactlastname sales_order_csv.contactfirstname        sales_order_csv.dealsize
Time taken: 3.115 seconds
hive>


Perform below menioned queries on "sales_order_orc" table :

a. Calculate total sales per year.

hive> select YEAR_ID as Year, sum(SALES) as Sales_Per_Year from sales_order_csv_orc group by YEAR_ID;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = root_20230620165207_d80ac028-69f4-47bf-b570-1c2d1105816b
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-06-20 16:52:09,174 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local2106512574_0003
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 1518148 HDFS Write: 795930 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
year    sales_per_year
2003    3516979.547241211
2004    4724162.593383789
2005    1791486.7086791992
Time taken: 1.359 seconds, Fetched: 3 row(s)
hive>

b. Find a product for which maximum orders were placed.

hive> select productcode from (select productcode, count(ordernumber) as total_orders from sales_order_csv_orc group by productcode order by total_orders desc limit 1)tab;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = root_20230620165530_3398a9c0-4f28-4bf0-92d7-b73c11e94aa2
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-06-20 16:55:32,184 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1129478082_0004
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-06-20 16:55:33,382 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_local1642244441_0005
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 1556650 HDFS Write: 795930 SUCCESS
Stage-Stage-2:  HDFS Read: 1556650 HDFS Write: 795930 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
productcode
S18_3232
Time taken: 2.649 seconds, Fetched: 1 row(s)
hive> 

c)Calculate the total sales for each quarter.

hive> select qtr_id as quarter, sum(sales) as total_sales from sales_order_csv_orc group by qtr_id;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = root_20230620165721_b01c1cf2-3582-4068-91cf-3c36f806da61
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-06-20 16:57:22,737 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1799925128_0006
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 1609472 HDFS Write: 795930 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
quarter total_sales
1       2350817.726501465
2       2048120.3029174805
3       1758910.808959961
4       3874780.010925293
Time taken: 1.363 seconds, Fetched: 4 row(s)

d) In which quarter sales was minimum.
hive> select Qtr_id as Quarter_with_min_sales from (select qtr_id, sum(sales) from sales_order_csv_orc group by qtr_id order by qtr_id limit 1)tab;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = root_20230620170646_4c79326a-0f0c-43d7-a889-df965aad012e
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-06-20 17:06:48,315 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local2072574879_0007
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-06-20 17:06:49,541 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_local2026590419_0008
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 1643848 HDFS Write: 795930 SUCCESS
Stage-Stage-2:  HDFS Read: 1643848 HDFS Write: 795930 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
quarter_with_min_sales
1
Time taken: 2.622 seconds, Fetched: 1 row(s)
hive>
e. In which country sales was maximum and in which country sales was minimum.

hive> select min(case when h=1 then country end)max_sale_country, min(case when l=1 then country end)min_sale_country from (select country, sum(sales), row_number() over(order by sum(sales))l, row_number() over(order by sum(sales) desc)h from sales_order_csv_orc group by country)tab;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = root_20230620170922_2cf17d7a-fce0-4f01-b0a4-61b6594e9cb1
Total jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-06-20 17:09:23,502 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1577682475_0009
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-06-20 17:09:24,704 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_local109664836_0010
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-06-20 17:09:25,875 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_local1855084101_0011
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-06-20 17:09:27,127 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_local1186080310_0012
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 1698452 HDFS Write: 795930 SUCCESS
Stage-Stage-2:  HDFS Read: 1698452 HDFS Write: 795930 SUCCESS
Stage-Stage-3:  HDFS Read: 1698452 HDFS Write: 795930 SUCCESS
Stage-Stage-4:  HDFS Read: 1698452 HDFS Write: 795930 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
max_sale_country        min_sale_country
USA     Ireland
Time taken: 5.041 seconds, Fetched: 1 row(s)
hive>

f. Calculate quartelry sales for each city.

hive> select city, qtr_id as quarter, sum(sales) as quarterly_sales from sales_order_csv_orc group by city, qtr_id;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = root_20230620171040_5a11ea8a-4c29-438d-b667-c8183dac24e6
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-06-20 17:10:42,173 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1017543926_0013
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 1754722 HDFS Write: 795930 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
city    quarter quarterly_sales
Bergamo 1       56181.320068359375
Boras   1       31606.72021484375
Brickhaven      1       31474.7802734375
Brisbane        1       16118.479858398438
Bruxelles       1       18800.089721679688
Burbank 1       37850.07958984375
Burlingame      1       13529.570190429688
Cambridge       1       21782.699951171875
Charleroi       1       16628.16015625
Cowes   1       26906.68017578125
Dublin  1       38784.470458984375
Espoo   1       51373.49072265625
Frankfurt       1       48698.82922363281
Gensve  1       50432.549560546875
Glendale        1       3987.199951171875
Graz    1       8775.159912109375
Helsinki        1       26422.819458007812
Kobenhavn       1       58871.110107421875
Lille   1       20178.1298828125
London  1       8477.219970703125
Los Angeles     1       23889.320068359375
Lule    1       9748.999755859375
Lyon    1       101339.13977050781
Madrid  1       357668.4899291992
Makati City     1       55245.02014160156
Manchester      1       51017.919860839844
Marseille       1       2317.43994140625
Melbourne       1       49637.57067871094
Minato-ku       1       38191.38977050781
NYC     1       32647.809814453125
Nantes  1       59617.39978027344
Nashua  1       12133.25
New Bedford     1       48578.95935058594
Newark  1       8722.1201171875
North Sydney    1       65012.41955566406
Osaka   1       50490.64013671875
Oulu    1       49055.40026855469
Paris   1       71494.17944335938
Pasadena        1       44273.359436035156
Philadelphia    1       27398.820434570312
Reims   1       52029.07043457031
San Diego       1       87489.23010253906
San Francisco   1       72899.19995117188
San Rafael      1       267315.2586669922
Singapore       1       28395.18994140625
South Brisbane  1       21730.029907226562
Stavern 1       54701.999755859375
Toulouse        1       15139.1201171875
Versailles      1       5759.419921875
Allentown       2       6166.7998046875
Barcelona       2       4219.2001953125
Boston  2       74994.240234375
Brickhaven      2       7277.35009765625
Bridgewater     2       75778.99060058594
Bruxelles       2       8411.949829101562
Cambridge       2       14380.920043945312
Charleroi       2       1711.260009765625
Chatswood       2       43971.429931640625
Espoo   2       31018.230102539062
Glen Waverly    2       14378.089965820312
Glendale        2       20350.949768066406
Kobenhavn       2       62091.880615234375
Las Vegas       2       33847.61975097656
Liverpool       2       91211.0595703125
London  2       32376.29052734375
Madrid  2       339588.0513305664
Marseille       2       52481.840087890625
Melbourne       2       60135.84033203125
Minato-ku       2       26482.700256347656
Montreal        2       58257.50012207031
NYC     2       165100.33947753906
Nantes  2       60344.990173339844
New Haven       2       36973.309814453125
Newark  2       74506.06909179688
Osaka   2       17114.43017578125
Oulu    2       17813.40008544922
Paris   2       80215.4203491211
Philadelphia    2       7287.240234375
Reggio Emilia   2       41509.94006347656
Reims   2       18971.959716796875
Salzburg        2       98104.24005126953
San Jose        2       160010.27026367188
San Rafael      2       7261.75
Singapore       2       92033.77014160156
Strasbourg      2       80438.47985839844
Tsawassen       2       31302.500244140625
Allentown       3       71930.61041259766
Bergen  3       16363.099975585938
Boras   3       53941.68981933594
Boston  3       15344.640014648438
Brickhaven      3       114974.53967285156
Brisbane        3       34100.030029296875
Bruxelles       3       47760.479736328125
Burlingame      3       42031.83020019531
Cambridge       3       48828.71942138672
Charleroi       3       1637.199951171875
Chatswood       3       69694.40002441406
Dublin  3       18971.959838867188
Espoo   3       31569.430053710938
Gensve  3       67281.00903320312
Glen Waverly    3       12334.819580078125
Glendale        3       7600.1201171875
Helsinki        3       42744.0595703125
Las Vegas       3       34453.84973144531
Madrid  3       69714.09008789062
Munich  3       34993.92004394531
NYC     3       63027.92004394531
Nantes  3       61310.880126953125
New Bedford     3       45738.38952636719
North Sydney    3       47191.76013183594
Oslo    3       34145.47021484375
Oulu    3       37501.580322265625
Paris   3       27798.480102539062
Pasadena        3       55776.119873046875
Reggio Emilia   3       56421.650390625
Reims   3       15146.31982421875
Salzburg        3       6693.2802734375
San Rafael      3       216297.40063476562
Singapore       3       90250.07995605469
South Brisbane  3       10640.290161132812
Torino  3       94117.25988769531
Toulouse        3       17251.08056640625
Tsawassen       3       43332.349609375
Aaarhus 4       100595.5498046875
Allentown       4       44040.729736328125
Barcelona       4       74192.66003417969
Bergamo 4       81774.40008544922
Bergen  4       95277.17993164062
Boras   4       48710.92053222656
Boston  4       63730.7802734375
Brickhaven      4       11528.52978515625
Bridgewater     4       26115.800537109375
Burbank 4       8234.559936523438
Burlingame      4       65221.67004394531
Cambridge       4       54251.659912109375
Charleroi       4       13463.480224609375
Chatswood       4       37905.14990234375
Cowes   4       51334.15966796875
Frankfurt       4       36472.76025390625
Glen Waverly    4       37878.54992675781
Glendale        4       34485.49987792969
Graz    4       43488.740234375
Helsinki        4       42083.499755859375
Kobenhavn       4       24078.610107421875
Koln    4       100306.58020019531
Las Vegas       4       14449.609741210938
Lille   4       48874.28088378906
Liverpool       4       26797.210083007812
London  4       83970.029296875
Los Angeles     4       24159.14013671875
Lule    4       66005.8798828125
Lyon    4       41535.11022949219
Madrid  4       315580.80963134766
Makati City     4       38770.71032714844
Manchester      4       106789.88977050781
Marseille       4       20136.859985351562
Melbourne       4       91221.99914550781
Minato-ku       4       55888.65026855469
Montreal        4       15947.290405273438
NYC     4       300011.6999511719
Nantes  4       23031.589599609375
Nashua  4       119552.04949951172
New Bedford     4       113557.509765625
New Haven       4       42498.760498046875
North Sydney    4       41791.949462890625
Oslo    4       45078.759765625
Paris   4       89436.60034179688
Pasadena        4       4512.47998046875
Philadelphia    4       116503.07043457031
Reggio Emilia   4       44669.740478515625
Reims   4       48895.59014892578
Salzburg        4       45001.10986328125
San Francisco   4       151459.4805908203
San Rafael      4       163983.64880371094
Sevilla 4       54723.621154785156
Singapore       4       77809.37023925781
South Brisbane  4       27098.800048828125
Stavern 4       61897.19006347656
Toulouse        4       38098.240234375
Vancouver       4       75238.91955566406
Versailles      4       59074.90026855469
White Plains    4       85555.98962402344
Time taken: 1.303 seconds, Fetched: 182 row(s)
hive>

h) Find a month for each year in which maximum number of quantities were sold.

hive> select year, month from (select year_id as year, month_id as month, sum(quantityordered) as total_quantity, dense_rank() over(partition by year_id order by sum(quantityordered) desc) as r from sales_order_csv_orc group by year_id, month_id)tab where r=1;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = root_20230620171148_13a78176-5d6b-468b-907a-e6adba94e50c
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-06-20 17:11:49,458 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1873109573_0014
Launching Job 2 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-06-20 17:11:50,674 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_local612397455_0015
MapReduce Jobs Launched:
Stage-Stage-1:  HDFS Read: 1793946 HDFS Write: 795930 SUCCESS
Stage-Stage-2:  HDFS Read: 1793946 HDFS Write: 795930 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
year    month
2003    11
2004    11
2005    5
Time taken: 2.619 seconds, Fetched: 3 row(s)
hive>






